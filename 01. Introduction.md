
# **1 Giới thiệu về Kubernetes**

Phần này bao gồm

* Thông tin giới thiệu về Kubernetes và nguồn gốc của nó
* Lý do tại sao Kubernetes được áp dụng rộng rãi
* Cách Kubernetes biến đổi trung tâm dữ liệu của bạn
* Tổng quan về kiến trúc và cách vận hành của nó
* Cách và thời điểm bạn nên tích hợp Kubernetes vào tổ chức của mình

Trước khi bạn có thể tìm hiểu về chi tiết của việc vận hành ứng dụng với Kubernetes, bạn phải trước hết nắm được những vấn đề mà Kubernetes được thiết kế để giải quyết, cách nó ra đời, và tác động của nó đến phát triển và triển khai ứng dụng. Phần đầu tiên này nhằm cung cấp một cái nhìn tổng quan về các chủ đề này.

---

### 1.1 Giới thiệu về Kubernetes

Từ “Kubernetes” trong tiếng Hy Lạp nghĩa là người điều khiển hoặc người lái tàu, người điều khiển con tàu — người đứng ở bánh lái (bánh lái tàu). Người lái tàu không nhất thiết là thuyền trưởng. Thuyền trưởng chịu trách nhiệm về con tàu, trong khi người lái tàu là người điều khiển nó.

Sau khi tìm hiểu thêm về những gì Kubernetes làm, bạn sẽ thấy rằng cái tên này thật sự rất phù hợp. Người lái tàu duy trì hướng đi của con tàu, thực hiện các mệnh lệnh do thuyền trưởng đưa ra và báo cáo lại hướng đi của con tàu. Kubernetes điều khiển các ứng dụng của bạn và báo cáo về trạng thái của chúng trong khi bạn — thuyền trưởng — quyết định nơi bạn muốn hệ thống đi đến.

**Cách phát âm Kubernetes và K8s là gì?**
Phát âm đúng trong tiếng Hy Lạp của Kubernetes, là *Kie-ver-nee-tees*, khác với cách phát âm tiếng Anh mà bạn thường nghe trong các cuộc trò chuyện kỹ thuật. Thường thì là *Koo-ber-netties* hoặc *Koo-ber-nay’-tace*, nhưng đôi khi bạn cũng có thể nghe thấy *Koo-ber-nets*, mặc dù hiếm.

Trong cả hội thoại viết và nói, nó cũng được gọi là **Kube** hoặc **K8s**, phát âm là *Kates*, trong đó số **8** đại diện cho số chữ cái bị lược bỏ giữa chữ cái đầu và chữ cái cuối.

---

### 1.1.1 Kubernetes trong một trang giấy

Kubernetes là một hệ thống phần mềm để tự động hóa việc triển khai và quản lý các hệ thống ứng dụng phức tạp, quy mô lớn được tạo thành từ các tiến trình máy tính chạy trong container. Hãy cùng tìm hiểu nó làm gì và làm như thế nào.

#### Trừu tượng hóa hạ tầng

Khi các nhà phát triển phần mềm hoặc người vận hành quyết định triển khai một ứng dụng, họ thực hiện điều này thông qua Kubernetes thay vì triển khai ứng dụng lên từng máy tính riêng lẻ. Kubernetes cung cấp một lớp trừu tượng trên phần cứng cơ sở cho cả người dùng và ứng dụng.

Như bạn có thể thấy trong hình dưới đây, hạ tầng cơ sở, nghĩa là máy tính, mạng và các thành phần khác, được ẩn khỏi các ứng dụng, giúp việc phát triển và cấu hình chúng trở nên dễ dàng hơn.

Hình 1.1 Trừu tượng hóa hạ tầng bằng Kubernetes

![](https://img001.prntscr.com/file/img001/qBa0To4KStCbP0EO-H210g.png)
---

#### Chuẩn hóa cách chúng ta triển khai ứng dụng

Bởi vì chi tiết của hạ tầng cơ sở không còn ảnh hưởng đến việc triển khai ứng dụng nữa, bạn triển khai ứng dụng vào trung tâm dữ liệu doanh nghiệp của mình theo cùng một cách như khi bạn làm trên đám mây. Một bản kê khai (manifest) duy nhất mô tả ứng dụng có thể được sử dụng cho triển khai cục bộ và cho việc triển khai trên bất kỳ nhà cung cấp đám mây nào. Tất cả sự khác biệt trong hạ tầng cơ sở đều được Kubernetes xử lý, vì vậy bạn có thể tập trung vào ứng dụng và logic nghiệp vụ bên trong nó.

---

#### Triển khai ứng dụng theo cách khai báo

Kubernetes sử dụng mô hình khai báo để định nghĩa một ứng dụng, như minh họa trong hình tiếp theo. Bạn mô tả các thành phần tạo nên ứng dụng của mình và Kubernetes biến mô tả này thành một ứng dụng đang chạy. Sau đó, nó giữ cho ứng dụng luôn khỏe mạnh bằng cách khởi động lại hoặc tạo lại các phần của nó khi cần thiết.

Hình 1.2 Mô hình khai báo của việc triển khai ứng dụng
![](https://img001.prntscr.com/file/img001/qBa0To4KStCbP0EO-H210g.png)
Bất cứ khi nào bạn thay đổi mô tả, Kubernetes sẽ thực hiện các bước cần thiết để cấu hình lại ứng dụng đang chạy sao cho phù hợp với mô tả mới, như minh họa trong hình tiếp theo.

Hình 1.3 Những thay đổi trong mô tả được phản ánh trong ứng dụng đang chạy

![](https://img001.prntscr.com/file/img001/3U5F5GOyQG-wMD-4cDNPhw.png)
---

#### Đảm nhận việc quản lý ứng dụng hàng ngày

Ngay khi bạn triển khai một ứng dụng lên Kubernetes, nó sẽ tiếp quản việc quản lý ứng dụng hàng ngày. Nếu ứng dụng gặp sự cố, Kubernetes sẽ tự động khởi động lại nó. Nếu phần cứng gặp sự cố hoặc cấu trúc hạ tầng thay đổi khiến ứng dụng cần được chuyển sang các máy khác, Kubernetes sẽ tự làm tất cả những việc này. Các kỹ sư chịu trách nhiệm vận hành hệ thống có thể tập trung vào bức tranh tổng thể thay vì lãng phí thời gian vào các chi tiết nhỏ nhặt.

Quay lại với phép ẩn dụ về đi biển: các kỹ sư phát triển và vận hành là các sĩ quan trên tàu đưa ra các quyết định ở tầm cao trong khi ngồi thoải mái trên ghế, và Kubernetes là người lái tàu đảm nhận các công việc ở tầm thấp để điều khiển hệ thống đi qua những vùng nước dữ dội mà các ứng dụng và hạ tầng của bạn đi qua.

Hình 1.4 Kubernetes tiếp quản việc quản lý ứng dụng
![](https://img001.prntscr.com/file/img001/_Wc7dfNJReSQxr74UECmkQ.png)
Tất cả những gì Kubernetes làm và tất cả những lợi ích mà nó mang lại đều cần có sự giải thích chi tiết hơn, mà chúng ta sẽ thảo luận sau. Trước khi làm điều đó, sẽ hữu ích nếu bạn biết nó bắt đầu như thế nào và dự án Kubernetes hiện đang ở đâu.

---

### 1.1.2 Về dự án Kubernetes

Kubernetes ban đầu được phát triển bởi Google. Google gần như luôn luôn chạy các ứng dụng trong container. Ngay từ năm 2014, đã có báo cáo rằng họ khởi tạo hai tỷ container mỗi tuần. Đó là hơn 3.000 container mỗi giây, và con số này ngày nay còn cao hơn nhiều. Họ chạy những container này trên hàng nghìn máy tính được phân bố trên hàng chục trung tâm dữ liệu trên khắp thế giới. Bây giờ hãy tưởng tượng làm tất cả điều này thủ công. Rõ ràng bạn cần tự động hóa, và ở quy mô khổng lồ này, nó phải hoàn hảo.

---

#### Về Borg và Omega – tiền thân của Kubernetes

Quy mô khổng lồ của khối lượng công việc của Google đã buộc họ phải phát triển các giải pháp để làm cho việc phát triển và quản lý hàng nghìn thành phần phần mềm có thể quản lý được và tiết kiệm chi phí. Qua nhiều năm, Google đã phát triển một hệ thống nội bộ gọi là **Borg** (và sau đó là một hệ thống mới gọi là **Omega**) giúp cả các nhà phát triển ứng dụng và người vận hành quản lý hàng nghìn ứng dụng và dịch vụ này.

Ngoài việc đơn giản hóa phát triển và quản lý, các hệ thống này cũng đã giúp họ đạt được việc sử dụng hạ tầng hiệu quả hơn. Điều này quan trọng trong bất kỳ tổ chức nào, nhưng khi bạn vận hành hàng trăm nghìn máy, ngay cả những cải tiến nhỏ trong việc sử dụng cũng có thể tiết kiệm hàng triệu đô la, vì vậy động lực để phát triển một hệ thống như vậy là rất rõ ràng.

---

**Lưu ý**
Dữ liệu về việc sử dụng năng lượng của Google cho thấy họ vận hành khoảng 900.000 máy chủ.

Theo thời gian, hạ tầng của bạn phát triển và thay đổi. Mỗi trung tâm dữ liệu mới đều là hiện đại nhất. Cơ sở hạ tầng của nó khác với những trung tâm được xây dựng trong quá khứ. Bất chấp sự khác biệt, việc triển khai ứng dụng trong một trung tâm dữ liệu không nên khác so với việc triển khai trong trung tâm dữ liệu khác. Điều này đặc biệt quan trọng khi bạn triển khai ứng dụng của mình trên nhiều vùng hoặc khu vực để giảm khả năng xảy ra sự cố khu vực gây ra thời gian ngừng hoạt động của ứng dụng. Để làm điều này hiệu quả, đáng giá để có một phương pháp triển khai nhất quán cho các ứng dụng của bạn.

---

#### Về Kubernetes – dự án mã nguồn mở – và các sản phẩm thương mại dựa trên nó

Dựa trên kinh nghiệm mà họ thu được khi phát triển Borg, Omega và các hệ thống nội bộ khác, năm 2014 Google giới thiệu Kubernetes, một dự án mã nguồn mở mà bây giờ mọi người đều có thể sử dụng và cải thiện thêm.

Hình 1.5 Nguồn gốc và trạng thái của dự án mã nguồn mở Kubernetes
![](https://img001.prntscr.com/file/img001/VsZVXk2cThq5CO3hG6MVmg.png)
Ngay khi Kubernetes được công bố, từ lâu trước khi phiên bản 1.0 chính thức được phát hành, các công ty khác như **Red Hat**, vốn luôn tiên phong trong phần mềm mã nguồn mở, đã nhanh chóng tham gia và giúp phát triển dự án. Nó cuối cùng đã phát triển vượt xa mong đợi của những người sáng lập, và ngày nay có thể nói là một trong những dự án mã nguồn mở hàng đầu thế giới, với hàng chục tổ chức và hàng nghìn cá nhân đóng góp cho nó.

Nhiều công ty hiện nay đang cung cấp các sản phẩm Kubernetes chất lượng doanh nghiệp được xây dựng từ dự án mã nguồn mở này. Chúng bao gồm **Red Hat OpenShift**, **Pivotal Container Service**, **Rancher** và nhiều sản phẩm khác.

---

#### Cách Kubernetes phát triển một hệ sinh thái cloud-native mới

Kubernetes cũng đã sinh ra nhiều dự án mã nguồn mở liên quan khác, hầu hết hiện nay thuộc sự quản lý của **Cloud Native Computing Foundation (CNCF)**, một phần của **Linux Foundation**.

CNCF tổ chức nhiều hội nghị **KubeCon - CloudNativeCon** mỗi năm tại Bắc Mỹ, Châu Âu và Trung Quốc. Năm 2019, tổng số người tham dự vượt quá 23.000, với **KubeCon Bắc Mỹ** đạt con số áp đảo 12.000 người tham dự. Những con số này cho thấy Kubernetes đã có tác động tích cực to lớn đến cách các công ty trên toàn thế giới triển khai ứng dụng ngày nay. Nó sẽ không được áp dụng rộng rãi như vậy nếu không phải như thế.

---

### 1.1.3 Hiểu tại sao Kubernetes lại phổ biến

Trong những năm gần đây, cách chúng ta phát triển ứng dụng đã thay đổi đáng kể. Điều này đã dẫn đến sự phát triển của các công cụ mới như Kubernetes, và đến lượt nó, đã thúc đẩy những thay đổi sâu hơn trong kiến trúc ứng dụng và cách chúng ta phát triển chúng. Hãy xem xét những ví dụ cụ thể sau.

---

#### Tự động hóa việc quản lý microservices

Trước đây, hầu hết các ứng dụng đều là các khối nguyên khối lớn. Các thành phần của ứng dụng được gắn chặt với nhau và tất cả chạy trong một tiến trình máy tính duy nhất. Ứng dụng được phát triển như một khối bởi một nhóm lớn các nhà phát triển và việc triển khai ứng dụng khá đơn giản. Bạn cài đặt nó trên một máy tính mạnh mẽ và cung cấp một chút cấu hình cần thiết. Việc mở rộng ứng dụng theo chiều ngang hiếm khi khả thi, vì vậy bất cứ khi nào bạn cần tăng dung lượng của ứng dụng, bạn phải nâng cấp phần cứng — nói cách khác, mở rộng ứng dụng theo chiều dọc.

Sau đó xuất hiện mô hình **microservices**. Các khối nguyên khối được chia thành hàng chục, đôi khi hàng trăm tiến trình riêng biệt, như minh họa trong hình sau. Điều này cho phép các tổ chức chia các bộ phận phát triển của họ thành các nhóm nhỏ hơn, nơi mỗi nhóm chỉ phát triển một phần của toàn bộ hệ thống — chỉ một số microservices.

Hình 1.6 So sánh ứng dụng nguyên khối với microservices
![](https://img001.prntscr.com/file/img001/gKsAI5f7SeaMj5OY6LjxDA.png)
Mỗi microservice giờ đây là một ứng dụng riêng biệt với chu kỳ phát triển và phát hành riêng. Sự phụ thuộc của các microservices khác nhau sẽ tất yếu phân nhánh theo thời gian. Một microservice yêu cầu một phiên bản của thư viện, trong khi một microservice khác yêu cầu một phiên bản khác, có thể không tương thích, của cùng thư viện đó. Việc chạy hai ứng dụng này trong cùng một hệ điều hành trở nên khó khăn.

May mắn thay, các container tự chúng giải quyết vấn đề này khi mỗi microservice yêu cầu một môi trường khác nhau, nhưng mỗi microservice giờ đây là một ứng dụng riêng biệt cần được quản lý riêng. Số lượng ứng dụng tăng lên khiến việc này trở nên khó khăn hơn rất nhiều.

Các phần riêng lẻ của toàn bộ ứng dụng không còn cần chạy trên cùng một máy tính nữa, điều này giúp dễ dàng mở rộng toàn bộ hệ thống hơn, nhưng cũng có nghĩa là các ứng dụng cần được cấu hình để giao tiếp với nhau.

Đối với các hệ thống chỉ có một vài thành phần, điều này thường có thể được thực hiện thủ công, nhưng hiện nay phổ biến khi thấy các triển khai có hơn một trăm microservices.

Khi hệ thống bao gồm nhiều microservices, quản lý tự động là rất quan trọng. Kubernetes cung cấp khả năng tự động hóa này. Các tính năng mà nó cung cấp làm cho nhiệm vụ quản lý hàng trăm microservices trở nên gần như tầm thường.

---

#### Thu hẹp khoảng cách giữa phát triển và vận hành

Cùng với những thay đổi này trong kiến trúc ứng dụng, chúng ta cũng đã thấy những thay đổi trong cách các nhóm phát triển và vận hành phần mềm. Trước đây, một nhóm phát triển xây dựng phần mềm một cách biệt lập và sau đó ném sản phẩm hoàn thiện qua bức tường cho nhóm vận hành, những người sẽ triển khai nó và quản lý nó từ đó.

Với sự xuất hiện của mô hình **DevOps**, hai nhóm giờ đây làm việc gần gũi hơn nhiều trong suốt vòng đời của sản phẩm phần mềm. Nhóm phát triển giờ đây tham gia nhiều hơn vào việc quản lý hàng ngày của phần mềm đã triển khai. Nhưng điều đó có nghĩa là họ giờ đây cần phải biết về hạ tầng mà nó đang chạy.

Là một nhà phát triển phần mềm, trọng tâm chính của bạn là triển khai logic nghiệp vụ. Bạn không muốn xử lý các chi tiết của các máy chủ cơ sở. May mắn thay, Kubernetes ẩn các chi tiết này.

---

#### Chuẩn hóa đám mây

Trong thập kỷ hoặc hai thập kỷ qua, nhiều tổ chức đã chuyển phần mềm của họ từ máy chủ cục bộ lên **đám mây**. Lợi ích của việc này dường như đã vượt qua nỗi sợ bị **khóa chặt** vào một nhà cung cấp đám mây cụ thể, điều này xảy ra khi dựa vào các API độc quyền của nhà cung cấp để triển khai và quản lý ứng dụng.

Bất kỳ công ty nào muốn có thể di chuyển các ứng dụng của mình từ nhà cung cấp này sang nhà cung cấp khác sẽ phải thực hiện các nỗ lực bổ sung, ban đầu là không cần thiết, để trừu tượng hóa hạ tầng và các API của nhà cung cấp đám mây cơ sở khỏi các ứng dụng. Điều này đòi hỏi tài nguyên mà lẽ ra có thể được tập trung vào việc xây dựng logic nghiệp vụ chính.

Kubernetes cũng đã giúp ích trong vấn đề này. Sự phổ biến của Kubernetes đã buộc tất cả các nhà cung cấp đám mây lớn phải tích hợp Kubernetes vào các dịch vụ của họ. Khách hàng giờ đây có thể triển khai ứng dụng lên bất kỳ nhà cung cấp đám mây nào thông qua một bộ API tiêu chuẩn do Kubernetes cung cấp.

Hình 1.7 Kubernetes đã chuẩn hóa cách bạn triển khai ứng dụng trên các nhà cung cấp đám mây
![](https://img001.prntscr.com/file/img001/P4xFpB3GTs-FtshoRg3pEg.png)
Nếu ứng dụng được xây dựng dựa trên các API của Kubernetes thay vì trực tiếp trên các API độc quyền của một nhà cung cấp đám mây cụ thể, nó có thể được chuyển đổi tương đối dễ dàng sang bất kỳ nhà cung cấp nào khác.


# **1.2 Hiểu về Kubernetes**
Phần trước đã giải thích về nguồn gốc của Kubernetes và lý do vì sao nó được áp dụng rộng rãi. Trong phần này, chúng ta sẽ xem xét kỹ hơn Kubernetes thực sự là gì.

---

### **1.2.1 Hiểu cách Kubernetes biến đổi một cụm máy tính**

Hãy xem xét kỹ hơn cách mà nhận thức về trung tâm dữ liệu thay đổi khi bạn triển khai Kubernetes trên các máy chủ của mình.

#### **Kubernetes giống như một hệ điều hành cho các cụm máy tính**

Ta có thể hình dung Kubernetes như một hệ điều hành cho cụm máy tính. Hình dưới đây minh họa sự tương đồng giữa hệ điều hành chạy trên một máy tính và Kubernetes chạy trên một cụm máy tính.

**Hình 1.8** Kubernetes đối với cụm máy tính cũng giống như hệ điều hành đối với một máy tính
![](https://img001.prntscr.com/file/img001/yZq4lVh0SLyNB-h9Hi933g.png)

Cũng như một hệ điều hành hỗ trợ các chức năng cơ bản của máy tính, chẳng hạn như lập lịch tiến trình trên CPU và đóng vai trò như giao diện giữa ứng dụng và phần cứng máy tính, Kubernetes lập lịch các thành phần của ứng dụng phân tán lên từng máy tính trong cụm và đóng vai trò như giao diện giữa ứng dụng và cụm.

Nó giải phóng các nhà phát triển ứng dụng khỏi việc phải triển khai các cơ chế liên quan đến hạ tầng trong ứng dụng của họ; thay vào đó, họ dựa vào Kubernetes để cung cấp các cơ chế đó. Bao gồm những thứ như:

* **service discovery (khám phá dịch vụ)** - cơ chế cho phép ứng dụng tìm các ứng dụng khác và sử dụng dịch vụ mà chúng cung cấp,
* **horizontal scaling (mở rộng theo chiều ngang)** - nhân bản ứng dụng để điều chỉnh theo biến động của tải,
* **load-balancing (cân bằng tải)** - phân phối tải trên tất cả các bản sao ứng dụng,
* **self-healing (tự phục hồi)** - giữ cho hệ thống khỏe mạnh bằng cách tự động khởi động lại các ứng dụng bị lỗi và di chuyển chúng sang các nút khỏe mạnh sau khi nút cũ bị lỗi,
* **leader election (bầu chọn leader)** - cơ chế quyết định phiên bản nào của ứng dụng sẽ hoạt động trong khi các phiên bản khác vẫn ở trạng thái chờ, sẵn sàng tiếp quản nếu phiên bản chính bị lỗi.

Bằng cách dựa vào Kubernetes để cung cấp các tính năng này, các nhà phát triển ứng dụng có thể tập trung vào việc triển khai logic nghiệp vụ cốt lõi thay vì lãng phí thời gian tích hợp ứng dụng với hạ tầng.

---

#### **Kubernetes phù hợp với một cụm máy tính như thế nào**

Để có một ví dụ cụ thể về cách Kubernetes được triển khai lên một cụm máy tính, hãy xem hình sau:

**Hình 1.9** Các máy tính trong một cụm Kubernetes được chia thành Control Plane và Workload Plane
![](https://img001.prntscr.com/file/img001/fcoF9eRvR-azPDrZFSOboA.png)
Bạn bắt đầu với một nhóm máy và chia chúng thành hai nhóm - **master node** và **worker node**. Các **master node** sẽ chạy **Kubernetes Control Plane**, đại diện cho “bộ não” của hệ thống và điều khiển cụm, trong khi các máy còn lại sẽ chạy các ứng dụng của bạn - gọi là **workloads** - và do đó đại diện cho **Workload Plane**.

**Lưu ý**
Workload Plane đôi khi còn được gọi là **Data Plane**, nhưng thuật ngữ này có thể gây nhầm lẫn vì mặt phẳng này không lưu trữ dữ liệu mà là chạy ứng dụng.
Cũng đừng bị nhầm lẫn bởi từ “plane” - trong ngữ cảnh này, bạn có thể hiểu nó là “bề mặt” mà các ứng dụng chạy trên đó.

Các cụm không phải sản xuất có thể chỉ sử dụng một master node, nhưng các cụm yêu cầu khả năng sẵn sàng cao sử dụng ít nhất ba master node vật lý để lưu trữ Control Plane. Số lượng worker node phụ thuộc vào số lượng ứng dụng mà bạn sẽ triển khai.

---

#### **Cách tất cả các nút trong cụm trở thành một khu vực triển khai lớn**

Sau khi Kubernetes được cài đặt trên các máy tính, bạn không còn cần phải quan tâm đến từng máy riêng lẻ khi triển khai ứng dụng nữa. Bất kể có bao nhiêu worker node trong cụm, tất cả chúng trở thành một **không gian duy nhất** để bạn triển khai ứng dụng. Bạn thực hiện việc này thông qua **Kubernetes API**, được cung cấp bởi **Kubernetes Control Plane**.

**Hình 1.10** Kubernetes hiển thị cụm dưới dạng một khu vực triển khai thống nhất
![](https://img001.prntscr.com/file/img001/lBxCWHsqTa2ZdlqWYh8JKg.png)
Khi tôi nói rằng tất cả worker node trở thành một không gian, tôi không có ý rằng bạn có thể triển khai một ứng dụng cực lớn trải rộng trên nhiều máy nhỏ. Kubernetes không làm những phép màu kiểu đó. Mỗi ứng dụng phải đủ nhỏ để phù hợp với một trong các worker node.

Ý tôi là khi triển khai ứng dụng, bạn không cần quan tâm nó sẽ chạy trên worker node nào. Kubernetes thậm chí có thể di chuyển ứng dụng từ node này sang node khác sau này. Bạn thậm chí có thể không nhận ra khi điều đó xảy ra, và bạn cũng không cần quan tâm.

---

### **1.2.2 Lợi ích của việc sử dụng Kubernetes**

Bạn đã biết tại sao nhiều tổ chức trên thế giới đã chào đón Kubernetes vào trung tâm dữ liệu của họ. Bây giờ, hãy xem xét kỹ hơn những lợi ích cụ thể mà nó mang lại cho cả đội phát triển và vận hành CNTT.

#### **Triển khai ứng dụng theo cơ chế tự phục vụ**

Vì Kubernetes trình bày tất cả các worker node như một bề mặt triển khai duy nhất, giờ đây không còn quan trọng là ứng dụng của bạn được triển khai lên node nào nữa. Điều này có nghĩa là các nhà phát triển có thể tự mình triển khai ứng dụng, ngay cả khi họ không biết gì về số lượng node hoặc đặc điểm của từng node.

Trong quá khứ, các quản trị viên hệ thống là người quyết định ứng dụng nên được đặt ở đâu. Nhiệm vụ này giờ đây được giao cho Kubernetes. Điều này cho phép nhà phát triển triển khai ứng dụng mà không cần dựa vào người khác. Khi một nhà phát triển triển khai ứng dụng, Kubernetes sẽ chọn node tốt nhất để chạy ứng dụng dựa trên yêu cầu tài nguyên của ứng dụng và tài nguyên có sẵn trên mỗi node.

---

#### **Giảm chi phí nhờ tận dụng hạ tầng tốt hơn**

Nếu bạn không quan tâm ứng dụng của mình chạy trên node nào, điều đó cũng có nghĩa là nó có thể được di chuyển đến bất kỳ node nào khác vào bất kỳ thời điểm nào mà bạn không cần lo lắng. Kubernetes có thể cần làm điều này để nhường chỗ cho một ứng dụng lớn hơn mà ai đó muốn triển khai.

Khả năng di chuyển ứng dụng này cho phép các ứng dụng được sắp xếp gọn gàng với nhau để tài nguyên của các node được sử dụng tối ưu nhất.

**Lưu ý**
Trong chương 17, bạn sẽ tìm hiểu thêm về cách Kubernetes quyết định nơi đặt từng ứng dụng và cách bạn có thể ảnh hưởng đến quyết định đó.

Việc tìm ra tổ hợp tối ưu có thể khó khăn và tốn thời gian, đặc biệt khi số lượng lựa chọn có thể có là rất lớn, chẳng hạn khi bạn có nhiều thành phần ứng dụng và nhiều node máy chủ để triển khai. Máy tính có thể thực hiện nhiệm vụ này tốt hơn và nhanh hơn con người rất nhiều. Kubernetes làm điều này rất tốt. Bằng cách kết hợp các ứng dụng khác nhau trên cùng một máy, Kubernetes cải thiện việc sử dụng hạ tầng phần cứng của bạn để bạn có thể chạy nhiều ứng dụng hơn trên ít máy chủ hơn.

---

#### **Tự động điều chỉnh theo tải thay đổi**

Sử dụng Kubernetes để quản lý các ứng dụng đã triển khai cũng có nghĩa là đội vận hành không cần phải liên tục giám sát tải của từng ứng dụng để phản ứng với các đỉnh tải đột ngột. Kubernetes cũng lo việc này.

Nó có thể giám sát tài nguyên mà mỗi ứng dụng tiêu thụ và các chỉ số khác, điều chỉnh số lượng phiên bản đang chạy của mỗi ứng dụng để xử lý tải tăng hoặc mức sử dụng tài nguyên tăng.

Khi bạn chạy Kubernetes trên hạ tầng đám mây, nó thậm chí có thể tăng kích thước của cụm bằng cách cung cấp thêm các node thông qua API của nhà cung cấp đám mây. Bằng cách này, bạn sẽ không bao giờ hết chỗ để chạy thêm phiên bản ứng dụng.

---

#### **Giữ cho ứng dụng chạy ổn định**

Kubernetes cũng nỗ lực hết sức để đảm bảo rằng các ứng dụng của bạn chạy ổn định. Nếu ứng dụng của bạn bị treo, Kubernetes sẽ tự động khởi động lại nó.

Ngay cả khi bạn có một ứng dụng bị lỗi và hết bộ nhớ sau vài giờ chạy, Kubernetes sẽ đảm bảo ứng dụng của bạn vẫn cung cấp dịch vụ cho người dùng bằng cách tự động khởi động lại trong trường hợp này.

Kubernetes là một hệ thống **tự phục hồi**, vì nó xử lý các lỗi phần mềm như ví dụ trên, nhưng nó cũng xử lý các lỗi phần cứng. Khi các cụm lớn dần lên, tần suất xảy ra lỗi node cũng tăng. Ví dụ, trong một cụm có 100 node với MTBF (thời gian trung bình giữa các lỗi) là 100 ngày cho mỗi node, bạn có thể kỳ vọng một node bị lỗi mỗi ngày.

Khi một node bị lỗi, Kubernetes tự động di chuyển ứng dụng sang các node còn khỏe mạnh. Đội vận hành không còn cần phải di chuyển ứng dụng thủ công nữa và có thể tập trung vào việc sửa chữa node bị lỗi để đưa nó trở lại nhóm tài nguyên phần cứng sẵn có.

Nếu hạ tầng của bạn có đủ tài nguyên trống để hệ thống hoạt động bình thường mà không cần node bị lỗi đó, đội vận hành thậm chí không cần phản ứng ngay lập tức. Nếu lỗi xảy ra vào giữa đêm, không ai trong đội vận hành cần phải thức dậy. Họ có thể ngủ ngon và xử lý node bị lỗi vào giờ làm việc bình thường.

---

#### **Đơn giản hóa phát triển ứng dụng**

Những cải tiến được mô tả ở phần trước chủ yếu liên quan đến việc triển khai ứng dụng. Nhưng còn quá trình phát triển ứng dụng thì sao? Kubernetes có mang lại gì không? Câu trả lời là **có**.

Như đã đề cập trước đó, Kubernetes cung cấp các dịch vụ liên quan đến hạ tầng mà nếu không có nó, bạn sẽ phải tự triển khai trong ứng dụng. Điều này bao gồm khám phá dịch vụ và/hoặc peer trong ứng dụng phân tán, bầu chọn leader, cấu hình tập trung cho ứng dụng và nhiều hơn nữa.

Kubernetes cung cấp điều này trong khi vẫn giữ cho ứng dụng **không phụ thuộc Kubernetes**, nhưng khi cần, ứng dụng cũng có thể truy vấn API của Kubernetes để lấy thông tin chi tiết về môi trường của chúng. Chúng cũng có thể sử dụng API để thay đổi môi trường.

---

### **1.2.3 Kiến trúc của một cụm Kubernetes**

Như bạn đã biết, một cụm Kubernetes bao gồm các node được chia thành hai nhóm:

* **Tập hợp các master node** lưu trữ các thành phần của **Control Plane**, đây là “bộ não” của hệ thống vì chúng điều khiển toàn bộ cụm.
* **Tập hợp các worker node** tạo thành **Workload Plane**, nơi các workloads (hay ứng dụng) của bạn chạy.

Hình dưới đây hiển thị hai plane và các node khác nhau mà chúng bao gồm.

**Hình 1.11** Hai plane tạo thành một cụm Kubernetes
![](https://img001.prntscr.com/file/img001/rKUhylyFSwqGVAKh0BS8Cw.png)
Hai plane, và do đó hai loại node, chạy các thành phần Kubernetes khác nhau. Hai phần tiếp theo của sách sẽ giới thiệu chúng và tóm tắt chức năng mà không đi sâu vào chi tiết. Các thành phần này sẽ được đề cập nhiều lần trong phần tiếp theo của sách, nơi tôi giải thích các khái niệm cơ bản của Kubernetes. Phần ba của sách sẽ đi sâu hơn vào các thành phần và nội bộ của chúng.

---

#### **Các thành phần của Control Plane**

Control Plane là nơi điều khiển cụm. Nó bao gồm nhiều thành phần chạy trên một master node hoặc được nhân bản trên nhiều master node để đảm bảo tính sẵn sàng cao. Các thành phần của Control Plane được hiển thị trong hình sau:

**Hình 1.12** Các thành phần của Kubernetes Control Plane
![](https://img001.prntscr.com/file/img001/ZKKjQ7v4Sz28TeL7-JxWNg.png)
Các thành phần và chức năng của chúng:

* **Kubernetes API Server**: Cung cấp **Kubernetes API** dạng RESTful. Các kỹ sư sử dụng cụm và các thành phần Kubernetes khác tạo đối tượng thông qua API này.
* **etcd distributed datastore**: Lưu trữ phân tán các đối tượng mà bạn tạo thông qua API, vì API Server bản thân nó là stateless. API Server là thành phần duy nhất giao tiếp với etcd.
* **Scheduler**: Quyết định mỗi phiên bản ứng dụng sẽ chạy trên worker node nào.
* **Controllers**: Kích hoạt các đối tượng mà bạn tạo thông qua API. Phần lớn chỉ tạo các đối tượng khác, nhưng một số cũng giao tiếp với các hệ thống bên ngoài (ví dụ: nhà cung cấp đám mây thông qua API của họ).

Các thành phần của Control Plane giữ và điều khiển trạng thái của cụm, nhưng chúng không chạy các ứng dụng của bạn. Việc này do các node (worker) thực hiện.

---

#### **Các thành phần trên worker node**

Các worker node là các máy tính chạy ứng dụng của bạn. Chúng tạo thành **Workload Plane** của cụm. Ngoài ứng dụng, một số thành phần của Kubernetes cũng chạy trên các node này. Chúng thực hiện nhiệm vụ chạy, giám sát và cung cấp khả năng kết nối giữa các ứng dụng của bạn.

**Hình 1.13** Các thành phần Kubernetes chạy trên mỗi node
![](https://img001.prntscr.com/file/img001/J9nRBmx4RWa_Lq_JDvQ68A.png)
Mỗi node chạy tập hợp các thành phần sau:

* **Kubelet**: Agent giao tiếp với API server và quản lý các ứng dụng chạy trên node của nó. Nó báo cáo trạng thái của các ứng dụng và node thông qua API.
* **Container Runtime**: Có thể là Docker hoặc bất kỳ runtime nào tương thích với Kubernetes. Nó chạy các ứng dụng của bạn trong container theo hướng dẫn của Kubelet.
* **Kubernetes Service Proxy (Kube Proxy)**: Cân bằng tải lưu lượng mạng giữa các ứng dụng. Tên của nó gợi ý rằng lưu lượng đi qua nó, nhưng điều đó không còn đúng nữa. Bạn sẽ tìm hiểu lý do trong chương 14.

---

#### **Các thành phần bổ sung**

Hầu hết các cụm Kubernetes cũng chứa một số thành phần khác. Bao gồm **DNS server**, **plugin mạng**, **tác nhân ghi log** và nhiều thành phần khác. Chúng thường chạy trên các worker node nhưng cũng có thể được cấu hình để chạy trên master.

---

#### **Hiểu sâu hơn về kiến trúc**

Hiện tại, tôi chỉ mong bạn quen thuộc sơ bộ với tên của các thành phần này và chức năng của chúng, vì tôi sẽ đề cập đến chúng nhiều lần trong các chương tiếp theo. Bạn sẽ học từng phần nhỏ về chúng trong các chương này, nhưng tôi sẽ giải thích chi tiết hơn trong **chương 14**.

Tôi không thích giải thích cách mọi thứ hoạt động cho đến khi tôi giải thích chúng làm gì và dạy bạn cách sử dụng chúng trước. Nó giống như học lái xe. Bạn không cần biết dưới nắp ca-pô có gì. Lúc đầu, bạn chỉ muốn học cách đi từ điểm A đến điểm B. Chỉ sau đó bạn mới quan tâm đến cách chiếc xe làm được điều này.

Biết những gì bên dưới nắp ca-pô một ngày nào đó có thể giúp bạn khởi động xe trở lại sau khi nó bị hỏng và bạn bị mắc kẹt bên đường. Tôi ghét phải nói điều này, nhưng bạn sẽ có nhiều khoảnh khắc như vậy khi làm việc với Kubernetes do độ phức tạp khủng khiếp của nó.

### **1.2.4 Cách Kubernetes chạy một ứng dụng**
Với cái nhìn tổng quan về các thành phần tạo nên Kubernetes, giờ tôi có thể giải thích cách triển khai một ứng dụng trong Kubernetes.

---

### **Định nghĩa ứng dụng của bạn**

Mọi thứ trong Kubernetes được biểu diễn bằng một **đối tượng (object)**. Bạn tạo và truy xuất các đối tượng này thông qua **Kubernetes API**.

Ứng dụng của bạn bao gồm một số loại đối tượng khác nhau:

* Một loại biểu diễn việc triển khai ứng dụng tổng thể,
* Một loại biểu diễn một phiên bản đang chạy của ứng dụng,
* Một loại biểu diễn dịch vụ do một tập hợp các phiên bản này cung cấp và cho phép truy cập chúng qua một địa chỉ IP duy nhất,
* Và nhiều loại khác nữa.

Tất cả các loại này được giải thích chi tiết trong phần hai của sách. Hiện tại, bạn chỉ cần biết rằng bạn định nghĩa ứng dụng của mình thông qua một số loại đối tượng. Các đối tượng này thường được định nghĩa trong một hoặc nhiều **tệp manifest** ở định dạng **YAML** hoặc **JSON**.



**Định nghĩa**
**YAML** ban đầu có nghĩa là *“Yet Another Markup Language”*, nhưng sau đó được đổi thành từ viết tắt đệ quy *“YAML Ain’t Markup Language”*. Đây là một cách để **tuần tự hóa (serialize)** một đối tượng thành tệp văn bản dễ đọc cho con người.

**Định nghĩa**
**JSON** là viết tắt của *JavaScript Object Notation*. Đây là một cách khác để tuần tự hóa một đối tượng, nhưng thích hợp hơn cho việc trao đổi dữ liệu giữa các ứng dụng.

---

Hình sau đây minh họa ví dụ triển khai một ứng dụng bằng cách tạo một manifest với hai **deployment** được công khai qua hai **service**.

 **Hình 1.14** Triển khai một ứng dụng vào Kubernetes

![](https://img001.prntscr.com/file/img001/zn8csE8tSG2M92EKofRndg.png)
---

Các hành động diễn ra khi bạn triển khai ứng dụng:

1. Bạn gửi **manifest** của ứng dụng đến **Kubernetes API**. API Server ghi các đối tượng được định nghĩa trong manifest vào **etcd**.
2. Một **controller** phát hiện các đối tượng mới được tạo và tạo thêm nhiều đối tượng mới – mỗi đối tượng cho một phiên bản ứng dụng.
3. **Scheduler** gán một node cho mỗi phiên bản.
4. **Kubelet** phát hiện rằng một phiên bản được gán cho node của Kubelet. Nó chạy phiên bản ứng dụng thông qua **Container Runtime**.
5. **Kube Proxy** phát hiện các phiên bản ứng dụng đã sẵn sàng chấp nhận kết nối từ client và cấu hình bộ **load balancer** cho chúng.
6. **Kubelet** và **Controllers** giám sát hệ thống và giữ cho các ứng dụng tiếp tục chạy.

Quy trình này được giải thích chi tiết hơn trong các phần tiếp theo, nhưng lời giải thích đầy đủ sẽ có trong **chương 14**, sau khi bạn đã quen thuộc với tất cả các loại đối tượng và controller liên quan.

---

### **Gửi ứng dụng đến API**

Sau khi bạn đã tạo tệp YAML hoặc JSON, bạn gửi tệp này đến API, thường thông qua **công cụ dòng lệnh Kubernetes** gọi là **kubectl**.

**Lưu ý**
Kubectl được phát âm là *kube-control*, nhưng một số người trong cộng đồng thích gọi nhẹ nhàng hơn là *kube-cuddle*. Một số khác thì gọi nó là *kube-C-T-L*.

**Kubectl** tách tệp thành các đối tượng riêng lẻ và tạo từng đối tượng bằng cách gửi yêu cầu **HTTP PUT** hoặc **POST** đến API, như thường thấy với các API RESTful. **API Server** xác thực các đối tượng và lưu trữ chúng trong **etcd datastore**. Ngoài ra, nó thông báo cho tất cả các thành phần quan tâm rằng các đối tượng này đã được tạo. **Controllers**, được giải thích ngay sau đây, là một trong những thành phần này.

---

### **Về các controller**

Hầu hết các loại đối tượng đều có một **controller** liên kết. Một controller quan tâm đến một loại đối tượng cụ thể. Nó chờ API Server thông báo rằng một đối tượng mới đã được tạo, sau đó thực hiện các thao tác để kích hoạt đối tượng đó.

Thông thường, controller chỉ đơn giản tạo các đối tượng khác thông qua cùng **Kubernetes API**. Ví dụ: controller chịu trách nhiệm cho **application deployment** tạo một hoặc nhiều đối tượng đại diện cho các phiên bản riêng lẻ của ứng dụng. Số lượng đối tượng được tạo bởi controller phụ thuộc vào số lượng **replica** được chỉ định trong đối tượng **application deployment**.

---

### **Về Scheduler**

**Scheduler** là một loại controller đặc biệt, chỉ có nhiệm vụ lập lịch các phiên bản ứng dụng lên **worker node**. Nó chọn **worker node** tốt nhất cho mỗi phiên bản ứng dụng mới và gán node đó cho phiên bản này bằng cách chỉnh sửa đối tượng qua API.

---

### **Về Kubelet và Container Runtime**

**Kubelet** chạy trên mỗi worker node cũng là một loại controller. Nhiệm vụ của nó là chờ các phiên bản ứng dụng được gán cho node nơi nó đang chạy và sau đó chạy ứng dụng. Việc này được thực hiện bằng cách hướng dẫn **Container Runtime** khởi động **container** của ứng dụng.

---

### **Về Kube Proxy**

Vì một **application deployment** có thể bao gồm nhiều phiên bản ứng dụng, nên cần một **load balancer** để công khai chúng qua một địa chỉ IP duy nhất. **Kube Proxy**, một controller khác chạy cùng **Kubelet**, chịu trách nhiệm thiết lập **load balancer** này.

---

### **Giữ cho ứng dụng luôn khỏe mạnh**

Khi ứng dụng đã được khởi chạy và chạy ổn định, **Kubelet** giữ cho ứng dụng khỏe mạnh bằng cách khởi động lại nó khi nó bị dừng. Nó cũng báo cáo trạng thái của ứng dụng bằng cách cập nhật đối tượng đại diện cho phiên bản ứng dụng.

Các **controller** khác giám sát các đối tượng này và đảm bảo rằng các ứng dụng được di chuyển đến các node khỏe mạnh nếu node của chúng bị lỗi.

Giờ bạn đã quen thuộc sơ bộ với kiến trúc và chức năng của Kubernetes. Bạn không cần hiểu hoặc ghi nhớ tất cả chi tiết ngay bây giờ, vì việc tiếp thu thông tin này sẽ dễ dàng hơn khi bạn học về từng loại đối tượng riêng lẻ và các **controller** kích hoạt chúng trong phần hai của sách.

---

## **1.3 Giới thiệu Kubernetes vào tổ chức của bạn**

Để kết thúc chương này, hãy xem những lựa chọn nào có sẵn nếu bạn quyết định giới thiệu Kubernetes vào môi trường CNTT của riêng mình.

---

### **1.3.1 Chạy Kubernetes tại chỗ và trên đám mây**

Nếu bạn muốn chạy ứng dụng trên Kubernetes, bạn phải quyết định xem mình muốn chạy chúng **tại chỗ (on-premises)**, trên hạ tầng của tổ chức, hay với một trong những nhà cung cấp đám mây lớn, hoặc có thể cả hai – trong một **giải pháp hybrid cloud (đám mây lai)**.

---

#### **Chạy Kubernetes tại chỗ**

Chạy Kubernetes trên hạ tầng của riêng bạn có thể là lựa chọn duy nhất nếu các quy định yêu cầu bạn chạy ứng dụng **tại chỗ**. Điều này thường có nghĩa là bạn sẽ phải tự quản lý Kubernetes, nhưng chúng ta sẽ nói về điều này sau.

Kubernetes có thể chạy trực tiếp trên **bare-metal** hoặc trong các **virtual machine** trong trung tâm dữ liệu của bạn. Trong cả hai trường hợp, bạn sẽ không thể mở rộng cụm dễ dàng như khi chạy trên các máy ảo do nhà cung cấp đám mây cung cấp.

---

#### **Triển khai Kubernetes trên đám mây**

Nếu bạn không có hạ tầng **on-premises**, bạn không còn lựa chọn nào khác ngoài việc chạy Kubernetes trên **cloud**. Lợi thế của việc này là bạn có thể mở rộng cụm bất cứ lúc nào khi cần thiết.

Như đã đề cập trước đó, **Kubernetes** có thể yêu cầu nhà cung cấp đám mây cung cấp thêm máy ảo khi kích thước hiện tại của cụm không còn đủ để chạy tất cả ứng dụng bạn muốn triển khai.

Khi số lượng **workload** giảm và một số **worker node** không còn workload nào chạy, **Kubernetes** có thể yêu cầu nhà cung cấp đám mây hủy các máy ảo này để giảm chi phí vận hành. Tính **linh hoạt** này chắc chắn là một trong những lợi ích chính của việc chạy Kubernetes trên **cloud**.

---

#### **Sử dụng giải pháp hybrid cloud**

Một lựa chọn phức tạp hơn là chạy Kubernetes **tại chỗ**, nhưng cũng cho phép nó mở rộng sang **cloud**. Bạn có thể cấu hình Kubernetes để cung cấp thêm **node** trong cloud nếu vượt quá dung lượng của trung tâm dữ liệu của bạn.

Bằng cách này, bạn có được lợi ích của cả hai bên:

* Phần lớn thời gian, ứng dụng của bạn chạy tại chỗ mà không tốn chi phí thuê máy ảo,
* Trong những giai đoạn tải cao chỉ xảy ra vài lần trong năm, ứng dụng của bạn có thể xử lý tải tăng thêm nhờ tài nguyên bổ sung trên **cloud**.

Nếu trường hợp sử dụng của bạn yêu cầu, bạn cũng có thể chạy một **cụm Kubernetes** trên nhiều **nhà cung cấp đám mây** hoặc kết hợp bất kỳ lựa chọn nào ở trên. Việc này có thể được thực hiện bằng một **control plane duy nhất** hoặc một control plane ở mỗi vị trí.

---

### **1.3.2 Tự quản lý Kubernetes hay không**

Nếu bạn đang cân nhắc giới thiệu Kubernetes vào tổ chức, câu hỏi quan trọng nhất cần trả lời là liệu bạn sẽ **tự quản lý Kubernetes** hay sử dụng **Kubernetes-as-a-Service**, nơi người khác quản lý cho bạn.

---

#### **Tự quản lý Kubernetes**

Nếu bạn đã chạy ứng dụng tại chỗ và có đủ phần cứng để chạy một cụm Kubernetes **sẵn sàng sản xuất**, phản xạ đầu tiên của bạn có thể là tự triển khai và quản lý nó.

Nếu bạn hỏi bất kỳ ai trong cộng đồng Kubernetes rằng đây có phải là một ý tưởng tốt không, bạn thường sẽ nhận được câu trả lời **“không”** rất dứt khoát.

**Hình 1.14** là một minh họa rất đơn giản về những gì xảy ra trong một cụm Kubernetes khi bạn triển khai một ứng dụng. Ngay cả hình đó cũng đủ làm bạn thấy e ngại. Kubernetes mang đến một lượng lớn độ phức tạp bổ sung. Bất kỳ ai muốn chạy một cụm Kubernetes đều phải hiểu rõ tường tận cách nó hoạt động bên trong.

Quản lý các cụm Kubernetes sẵn sàng cho sản xuất là một **ngành công nghiệp trị giá hàng tỷ đô**. Trước khi bạn quyết định tự quản lý, điều quan trọng là phải tham khảo ý kiến của các kỹ sư đã từng làm điều này để tìm hiểu về các vấn đề mà hầu hết các nhóm gặp phải. Nếu không, bạn có thể đang tự đưa mình vào thất bại.

Mặt khác, thử nghiệm Kubernetes cho các trường hợp **phi sản xuất** hoặc sử dụng cụm Kubernetes **được quản lý** sẽ ít rủi ro hơn nhiều.

---

#### **Sử dụng cụm Kubernetes được quản lý trên đám mây**

Sử dụng Kubernetes dễ gấp mười lần so với quản lý nó. Hầu hết các nhà cung cấp đám mây lớn hiện nay đều cung cấp **Kubernetes-as-a-Service**. Họ quản lý Kubernetes và các thành phần của nó, trong khi bạn chỉ đơn giản sử dụng **Kubernetes API** như bất kỳ API nào khác mà nhà cung cấp đám mây cung cấp.

Các dịch vụ Kubernetes được quản lý hàng đầu bao gồm:

* **Google Kubernetes Engine (GKE)**
* **Azure Kubernetes Service (AKS)**
* **Amazon Elastic Kubernetes Service (EKS)**
* **IBM Cloud Kubernetes Service**
* **Red Hat OpenShift Online và Dedicated**
* **VMware Cloud PKS**
* **Alibaba Cloud Container Service for Kubernetes (ACK)**

Nửa đầu của cuốn sách này tập trung vào việc **sử dụng Kubernetes**. Bạn sẽ chạy các bài thực hành trên một **cụm phát triển cục bộ** và một **cụm GKE được quản lý**, vì tôi thấy nó dễ sử dụng nhất và mang lại trải nghiệm người dùng tốt nhất.

Phần thứ hai tiếp sẽ cung cấp cho bạn nền tảng vững chắc để **quản lý Kubernetes**, nhưng để thực sự thành thạo, bạn sẽ cần có thêm kinh nghiệm thực tế.


### **1.3.3 Sử dụng Kubernetes nguyên bản hay mở rộng**
Câu hỏi cuối cùng là nên sử dụng phiên bản mã nguồn mở nguyên bản của Kubernetes hay một sản phẩm Kubernetes mở rộng, chất lượng doanh nghiệp.

**Sử dụng phiên bản Kubernetes nguyên bản**
Phiên bản mã nguồn mở của Kubernetes được cộng đồng duy trì và đại diện cho sự phát triển tiên tiến nhất của Kubernetes. Điều này cũng có nghĩa là nó có thể không ổn định bằng các lựa chọn khác. Nó cũng có thể thiếu các thiết lập mặc định an toàn. Triển khai phiên bản nguyên bản yêu cầu rất nhiều tinh chỉnh để thiết lập mọi thứ cho mục đích sử dụng trong môi trường sản xuất.

**Sử dụng các bản phân phối Kubernetes cấp doanh nghiệp**
Một lựa chọn tốt hơn để sử dụng Kubernetes trong môi trường sản xuất là sử dụng bản phân phối Kubernetes cấp doanh nghiệp như OpenShift hoặc Rancher. Ngoài việc tăng cường bảo mật và hiệu suất nhờ các thiết lập mặc định tốt hơn, chúng còn cung cấp thêm các loại đối tượng ngoài những gì được cung cấp trong API Kubernetes gốc.

Ví dụ, Kubernetes nguyên bản không chứa các loại đối tượng đại diện cho người dùng trong cụm, trong khi các bản phân phối thương mại thì có. Chúng cũng cung cấp thêm các công cụ phần mềm để triển khai và quản lý các ứng dụng bên thứ ba phổ biến trên Kubernetes.

Tất nhiên, việc mở rộng và tăng cường bảo mật cho Kubernetes cần thời gian, vì vậy các bản phân phối Kubernetes thương mại thường chậm hơn một hoặc hai phiên bản so với phiên bản Kubernetes gốc. Điều này không quá tệ như bạn tưởng. Lợi ích mang lại thường vượt trội so với những hạn chế.

---

### **1.3.4 Bạn có nên sử dụng Kubernetes hay không?**
Tôi hy vọng chương này đã khiến bạn hào hứng với Kubernetes và bạn không thể chờ đợi để tích hợp nó vào hạ tầng CNTT của mình. Nhưng để kết thúc chương này một cách đầy đủ, chúng ta cần nói một hoặc hai điều về những trường hợp khi việc giới thiệu Kubernetes không phải là ý tưởng hay.

**Khối lượng công việc của bạn có cần quản lý tự động không?**
Điều đầu tiên bạn cần thành thật với chính mình là liệu bạn có thực sự cần tự động hóa việc quản lý các ứng dụng của mình hay không. Nếu ứng dụng của bạn là một khối nguyên khối lớn (monolith), bạn chắc chắn không cần Kubernetes.

Ngay cả khi bạn triển khai microservices, việc sử dụng Kubernetes có thể không phải là lựa chọn tốt nhất, đặc biệt nếu số lượng microservices của bạn rất nhỏ. Rất khó để đưa ra một con số chính xác khi nào nên chuyển đổi, vì còn có các yếu tố khác ảnh hưởng đến quyết định.

Nhưng nếu hệ thống của bạn có ít hơn năm microservices, thì việc thêm Kubernetes vào hỗn hợp này có lẽ không phải là ý tưởng tốt. Nếu hệ thống của bạn có hơn hai mươi microservices, bạn rất có thể sẽ được hưởng lợi từ việc tích hợp Kubernetes. Nếu số lượng microservices của bạn nằm ở mức trung gian, các yếu tố khác, như những yếu tố được mô tả tiếp theo, cần được xem xét.

**Bạn có đủ khả năng đầu tư thời gian của kỹ sư để học Kubernetes không?**
Kubernetes được thiết kế để cho phép các ứng dụng chạy mà không cần biết rằng chúng đang chạy trong Kubernetes. Mặc dù bản thân ứng dụng không cần được chỉnh sửa để chạy trong Kubernetes, các kỹ sư phát triển sẽ chắc chắn phải dành nhiều thời gian để học cách sử dụng Kubernetes, mặc dù chỉ có đội ngũ vận hành mới thực sự cần kiến thức này.

Thật khó để nói với các nhóm rằng bạn đang chuyển sang Kubernetes và mong đợi chỉ có đội vận hành mới bắt đầu tìm hiểu nó. Các nhà phát triển thích những thứ mới mẻ, hào nhoáng. Tại thời điểm viết sách này, Kubernetes vẫn còn là một thứ rất mới mẻ.

**Bạn có sẵn sàng cho chi phí tăng thêm trong giai đoạn đầu không?**
Mặc dù Kubernetes giảm chi phí vận hành dài hạn, nhưng việc giới thiệu Kubernetes vào tổ chức ban đầu sẽ liên quan đến chi phí tăng thêm cho đào tạo, thuê kỹ sư mới, xây dựng và mua sắm công cụ mới và có thể là cả phần cứng bổ sung. Kubernetes yêu cầu thêm tài nguyên tính toán ngoài tài nguyên mà ứng dụng sử dụng.

**Đừng tin vào sự thổi phồng**
Mặc dù Kubernetes đã xuất hiện được vài năm tại thời điểm viết cuốn sách này, tôi không thể nói rằng giai đoạn thổi phồng đã qua. Sự phấn khích ban đầu chỉ mới bắt đầu lắng xuống, nhưng nhiều kỹ sư có thể vẫn chưa thể đưa ra quyết định hợp lý về việc tích hợp Kubernetes có thực sự cần thiết như vẻ ngoài của nó hay không.

---

## **1.4 Tóm tắt**
Trong chương giới thiệu này, bạn đã biết rằng:

* Kubernetes trong tiếng Hy Lạp có nghĩa là người lái tàu. Giống như thuyền trưởng giám sát con tàu trong khi người lái tàu điều khiển nó, bạn giám sát cụm máy tính của mình trong khi Kubernetes thực hiện các tác vụ quản lý hàng ngày.
* Kubernetes được phát âm là “koo-ber-netties”. Kubectl, công cụ dòng lệnh của Kubernetes, được phát âm là “kube-control”.
* Kubernetes là một dự án mã nguồn mở được xây dựng dựa trên kinh nghiệm rộng lớn của Google trong việc vận hành các ứng dụng ở quy mô toàn cầu. Hàng nghìn cá nhân hiện đang đóng góp cho dự án này.
* Kubernetes sử dụng mô hình khai báo để mô tả việc triển khai ứng dụng. Sau khi bạn cung cấp mô tả về ứng dụng của mình cho Kubernetes, nó sẽ làm cho ứng dụng đó hoạt động.
* Kubernetes giống như một hệ điều hành cho cụm. Nó trừu tượng hóa hạ tầng và trình bày tất cả các máy tính trong trung tâm dữ liệu như một khu vực triển khai lớn, liền mạch.
* Các ứng dụng dựa trên microservices khó quản lý hơn các ứng dụng nguyên khối. Số lượng microservices càng nhiều, bạn càng cần tự động hóa việc quản lý chúng bằng một hệ thống như Kubernetes.
* Kubernetes giúp cả nhóm phát triển và nhóm vận hành làm tốt nhất công việc của họ. Nó giải phóng họ khỏi những tác vụ nhàm chán và giới thiệu một cách tiêu chuẩn để triển khai ứng dụng cả tại chỗ và trên bất kỳ đám mây nào.
* Sử dụng Kubernetes cho phép nhà phát triển triển khai ứng dụng mà không cần sự hỗ trợ của quản trị viên hệ thống. Nó giảm chi phí vận hành thông qua việc sử dụng hiệu quả phần cứng hiện có, tự động điều chỉnh hệ thống của bạn với sự dao động tải, và tự khôi phục chính nó cùng các ứng dụng đang chạy trên đó.
* Một cụm Kubernetes bao gồm các nút điều khiển (master) và các nút làm việc (worker). Các nút điều khiển chạy Control Plane, điều khiển toàn bộ cụm, trong khi các nút làm việc chạy các ứng dụng hoặc khối lượng công việc được triển khai, và do đó đại diện cho Workload Plane.
* Sử dụng Kubernetes thì đơn giản, nhưng quản lý nó thì khó. Một nhóm thiếu kinh nghiệm nên sử dụng dịch vụ Kubernetes-as-a-Service thay vì tự triển khai Kubernetes.

Cho đến giờ, bạn chỉ mới quan sát con tàu từ bến cảng. Đã đến lúc bạn bước lên tàu. Nhưng trước khi rời bến, bạn nên kiểm tra các container hàng hóa mà nó đang chở. Bạn sẽ làm điều đó tiếp theo.

